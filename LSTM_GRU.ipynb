{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwBLml47TYzc",
    "colab_type": "text"
   },
   "source": [
    "# Deep Learning\n",
    "\n",
    "***Name***: *Aviv Nutovitz*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjyPMg_aTpXT",
    "colab_type": "text"
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rvS8vMmVTr6C",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35.0
    },
    "outputId": "e207bc6a-3aed-49ad-ee07-07da94b6f8dc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.545069910502E12,
     "user_tz": -120.0,
     "elapsed": 1794.0,
     "user": {
      "displayName": "Aviv Notovich",
      "photoUrl": "",
      "userId": "02968534924654162048"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Embedding, LSTM, GRU, TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import Adam, SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import os, time\n",
    "from google.colab import files\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "SAVE_RESULTS_TO_DRIVE = True\n",
    "FOLDER_ID = 'XXX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wcc3E9xRTzMy",
    "colab_type": "code",
    "outputId": "d5f5f0c9-7007-4724-fa2f-58266572cd2b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.545070052549E12,
     "user_tz": -120.0,
     "elapsed": 139605.0,
     "user": {
      "displayName": "Aviv Notovich",
      "photoUrl": "",
      "userId": "02968534924654162048"
     }
    },
    "colab": {
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "ok": true,
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "status": 200.0,
       "status_text": ""
      }
     },
     "base_uri": "https://localhost:8080/",
     "height": 143.0
    }
   },
   "outputs": [],
   "source": [
    "# first upload the data files from your local file system:\n",
    "uploadedFiles = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dS7qZ8PrT3o_",
    "colab_type": "text"
   },
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "K9QlqVwYT467",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "#class Model:\n",
    "class Model:\n",
    "  \"\"\"\n",
    "  modelTypeDict - dictionary with the type of regularization and the reuquired parameters: \n",
    "  1. {\"L\":\"none\"} (LSTM based network without dropout)\n",
    "  2. {\"LD\":\"0.5\"} (LSTM based network with dropout)\n",
    "  3. {\"G\":\"none\"} (GRU based network without dropout)\n",
    "  4. {\"GD\":\"0.5\"} (GRU based network with dropout)\n",
    "  \"\"\"\n",
    "  @staticmethod\n",
    "  def build(model_type_dict, vocabulary, hidden_size=200, num_steps=20):\n",
    "    # initialize the model\n",
    "    model = Sequential()\n",
    "    # parameters for the diffrent types of regularization\n",
    "    type_key = next(iter(model_type_dict))\n",
    "    type_value = model_type_dict[type_key]\n",
    "    \n",
    "    # embedding layer\n",
    "    model.add(Embedding(vocabulary, hidden_size, input_length=num_steps))\n",
    "    \n",
    "    # for LSTM model\n",
    "    if 'L' in type_key:\n",
    "      # first layer\n",
    "      model.add(LSTM(hidden_size, return_sequences=True))\n",
    "      # add dropouts\n",
    "      if 'D' in type_key:\n",
    "        model.add(Dropout(float(type_value)))\n",
    "      # scound layer\n",
    "      model.add(LSTM(hidden_size, return_sequences=True))\n",
    "      # add dropouts\n",
    "      if 'D' in type_key:\n",
    "        model.add(Dropout(float(type_value)))\n",
    "    # for GRU model\n",
    "    else:\n",
    "      # first layer\n",
    "      model.add(GRU(hidden_size, return_sequences=True))\n",
    "      # add dropouts\n",
    "      if 'D' in type_key:\n",
    "        model.add(Dropout(float(type_value)))\n",
    "      # scound layer\n",
    "      model.add(GRU(hidden_size, return_sequences=True))\n",
    "      # add dropouts\n",
    "      if 'D' in type_key:\n",
    "        model.add(Dropout(float(type_value)))\n",
    "    \n",
    "    model.add(TimeDistributed(Dense(vocabulary)))\n",
    "    model.add(Activation('softmax'))\n",
    "    optimizer = Adam(0.0005, clipnorm=10)\n",
    "#     optimizer = Nadam(lr=0.1, clipnorm=10)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['categorical_accuracy'])\n",
    "    return model\n",
    "    \n",
    "def build_model(model_type, vocabulary):\n",
    "  model = Model.build(model_type, vocabulary)\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBCx4Q-fUEOz",
    "colab_type": "text"
   },
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "5jIoJoOjmZlZ",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "\n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary\n",
    "        # this will track the progress of the batches sequentially through the\n",
    "        # data set - once the data reaches the end of the data set it will reset\n",
    "        # back to zero\n",
    "        self.current_idx = 0\n",
    "        # skip_step is the number of words which will be skipped before the next\n",
    "        # batch is skimmed from the data set\n",
    "        self.skip_step = skip_step\n",
    "\n",
    "    def generate(self):\n",
    "        x = np.zeros((self.batch_size, self.num_steps))\n",
    "        y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.current_idx + self.num_steps >= len(self.data):\n",
    "                    # reset the index back to the start of the data set\n",
    "                    self.current_idx = 0\n",
    "                x[i, :] = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
    "                temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_steps + 1]\n",
    "                # convert all of temp_y into a one hot representation\n",
    "                y[i, :, :] = to_categorical(temp_y, num_classes=self.vocabulary)\n",
    "                self.current_idx += self.skip_step\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "nRLt46Q2H4rb",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        return f.read().replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "def build_vocab(filename):\n",
    "    data = read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "    return word_to_id\n",
    "\n",
    "def file_to_word_ids(filename, word_to_id):\n",
    "    data = read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "  \n",
    "def load_data():\n",
    "  # build the complete vocabulary, then convert text data to list of integers\n",
    "  train_path = 'ptb.train.txt'\n",
    "  valid_path = 'ptb.valid.txt'\n",
    "  test_path = 'ptb.test.txt'\n",
    "\n",
    "  word_to_id = build_vocab(train_path)\n",
    "  train_data = file_to_word_ids(train_path, word_to_id)\n",
    "  valid_data = file_to_word_ids(valid_path, word_to_id)\n",
    "  test_data = file_to_word_ids(test_path, word_to_id)\n",
    "  vocabulary = len(word_to_id)\n",
    "  reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "\n",
    "  return train_data, valid_data, test_data, vocabulary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9wJlCEDD3wa",
    "colab_type": "text"
   },
   "source": [
    "###Drive REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "g4u7EIecD9LP",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def set_drive_api():\n",
    "  # connect to drive\n",
    "  !pip install -U -q PyDrive\n",
    "  from pydrive.auth import GoogleAuth\n",
    "  from pydrive.drive import GoogleDrive\n",
    "  from google.colab import auth\n",
    "  from oauth2client.client import GoogleCredentials\n",
    "  from google.colab import files\n",
    "\n",
    "  # 1. Authenticate and create the PyDrive client.\n",
    "  auth.authenticate_user()\n",
    "  gauth = GoogleAuth()\n",
    "  gauth.credentials = GoogleCredentials.get_application_default()\n",
    "  drive = GoogleDrive(gauth)\n",
    "\n",
    "  # PyDrive reference:\n",
    "  # https://gsuitedevs.github.io/PyDrive/docs/build/html/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1r-g-JiQA-tO",
    "colab_type": "text"
   },
   "source": [
    "### Save Model & Results to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Ouq4wyHZBKhi",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def save_model_to_drive(net_type, model):\n",
    "  # parameters\n",
    "  net_type_to_save = next(iter(net_type))\n",
    "  full_model_name = \"model_{}.h5\".format(net_type_to_save)\n",
    "  # create is localy on the remote machine \n",
    "  model.save(full_model_name)\n",
    "  # save to drive\n",
    "  save(full_model_name)\n",
    "  print(\"Saved full model {} to disk\".format(net_type_to_save))\n",
    "\n",
    "def save_resluts_to_drive_as_txt(net_type, val_categorical_accuracy_list, val_loss_list):\n",
    "  net_type_to_save = next(iter(net_type))\n",
    "  model_results = \"model_results_{}.txt\".format(net_type_to_save)\n",
    "  results_df = pd.DataFrame({'loss': t_loss_list, 'acc': t_acc_list, 'epochs': t_epochs_list, 'val_loss':val_loss_list, 'val_acc':val_categorical_accuracy_list})\n",
    "  # write results locally\n",
    "  results_df.to_csv(model_results, header=None, index=None, sep=' ')\n",
    "  save(model_results)\n",
    "  \n",
    "def save(file_name):\n",
    "  # 2. Create & upload a file json file to drive\n",
    "  uploaded = drive.CreateFile({'title': file_name, \"parents\": [{\"kind\": \"drive#fileLink\",\"id\": FOLDER_ID}]})\n",
    "  uploaded.SetContentFile(file_name)\n",
    "  uploaded.Upload()\n",
    "  print('Uploaded file with ID {}'.format(uploaded.get('id')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1LTELAMBsfr",
    "colab_type": "text"
   },
   "source": [
    "###Upload Saved Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "pV_-QrQ9B0QK",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def load_model_from_drive(net_type):\n",
    "  net_type_to_save = next(iter(net_type))\n",
    "  model_file_name = 'model_{}.h5'.format(net_type_to_save)\n",
    "  # download form drive\n",
    "  dowanload_file_drive_by_file_name(model_file_name)\n",
    "  new_model = keras.models.load_model(model_file_name)\n",
    "  return new_model\n",
    "  \n",
    "def load_resluts_from_drive_as_txt(net_type):\n",
    "  net_type_to_save = next(iter(net_type))\n",
    "  model_file_name = 'model_results_{}.txt'.format(net_type_to_save)\n",
    "  # download form drive\n",
    "  dowanload_file_drive_by_file_name(model_file_name)\n",
    "  # build the data frame\n",
    "  data = pd.read_csv(model_file_name, sep=\" \", header=None)\n",
    "  data.columns = [\"acc\", \"epochs\", \"loss\", val_loss_list, val_loss_list]\n",
    "  return data\n",
    "\n",
    "def dowanload_file_drive_by_file_name(file_name):\n",
    "  file_list = drive.ListFile({'q': \"'\"+FOLDER_ID+\"' in parents and trashed=false\"}).GetList()\n",
    "  for file1 in file_list:\n",
    "    if file1['title'] == file_name:\n",
    "      print('title: %s, id: %s' % (file1['title'], file1['id']))\n",
    "      downloaded = drive.CreateFile({'id': file1['id']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3x7fxm3jUHAT",
    "colab_type": "text"
   },
   "source": [
    "### Train Model and Plot Graphs:\n",
    "- Train model\n",
    "- Evaluate using test set\n",
    "- Saving the model to be loaded for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "VfNl3j-fWqhL",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "t_loss_list, t_acc_list, t_epochs_list = [], [], []\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, t_data):\n",
    "        self.t_data = t_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        print(\"\\nEvaluating loss on train at the end of epoch \" + str(int(epoch) + 1) + \"...\")\n",
    "        t1 = time.time()\n",
    "        steps_per_epoch = len(self.t_data)//(batch_size*num_steps)\n",
    "        data_generator = BatchGenerator(self.t_data, num_steps, batch_size, vocabulary)\n",
    "        loss, acc = self.model.evaluate_generator(data_generator.generate(), steps=steps_per_epoch)\n",
    "        t_loss_list.append(loss)\n",
    "        t_acc_list.append(acc)\n",
    "        t_epochs_list.append(epoch)\n",
    "        print('Loss: {}, acc: {}'.format(loss, acc))\n",
    "        t2 = time.time()\n",
    "        print(\"Total evaluation time: %0.2fs\\n\" % (t2 - t1))\n",
    "\n",
    "\n",
    "# train model\n",
    "def train_type(net_type, train_data, valid_data, num_steps, batch_size, vocabulary, epochs=20):\n",
    "  \n",
    "  np.random.seed(2018)\n",
    "  tf.set_random_seed(2018)\n",
    "  \n",
    "  # build & compile the model for the training\n",
    "  model = build_model(net_type, vocabulary)\n",
    "  \n",
    "  checkpoint_path = \"training_{}/cp.ckpt\".format(next(iter(net_type)))\n",
    "  checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "  # create dir if not exsit\n",
    "  !mkdir {checkpoint_dir}\n",
    "  \n",
    "  # Create checkpoint callback\n",
    "  cp_callback = ModelCheckpoint(checkpoint_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "  t_callback = TestCallback(train_data)\n",
    "  \n",
    "  train_data_generator = BatchGenerator(train_data, num_steps, batch_size, vocabulary, skip_step=num_steps)\n",
    "  valid_data_generator = BatchGenerator(valid_data, num_steps, batch_size, vocabulary, skip_step=num_steps)\n",
    "  \n",
    "  # Measure training time\n",
    "  t3 = time.time() \n",
    "  \n",
    "  history = model.fit_generator(train_data_generator.generate(), \n",
    "                                len(train_data)//(batch_size*num_steps), epochs,\n",
    "                                validation_data=valid_data_generator.generate(),\n",
    "                                validation_steps=len(valid_data)//(batch_size*num_steps), \n",
    "                                callbacks=[cp_callback, t_callback])\n",
    "  \n",
    "  # history results \n",
    "  val_categorical_accuracy_list = history.history['val_categorical_accuracy']\n",
    "  val_loss_list = history.history['val_loss']\n",
    "  \n",
    "  # save results to drive\n",
    "  if SAVE_RESULTS_TO_DRIVE:\n",
    "    set_drive_api()\n",
    "    save_model_to_drive(net_type, model)\n",
    "    save_resluts_to_drive_as_txt(net_type, val_loss_list, val_categorical_accuracy_list)\n",
    "  \n",
    "  t4 = time.time()\n",
    "  print(\"Total training time: %0.2fs\" % (t4 - t3))\n",
    "  \n",
    "  ## return the saved model path and history\n",
    "  return checkpoint_path, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "FIMQiGqIC1YS",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# plot graphs\n",
    "def plot_graphs(net_type):\n",
    "  # get dataframe form drive\n",
    "  df = load_resluts_from_drive_as_txt(net_type)\n",
    "  # set results to lists\n",
    "  t_loss_list, t_acc_list, t_epochs_list, val_categorical_accuracy_list, val_loss_list = df[\"loss\"], df[\"acc\"], df[\"epochs\"], df[\"val_acc\"], df[\"val_loss\"]\n",
    " \n",
    "  val_perplexity_lst = [math.exp(x) for x in val_loss_list]\n",
    "  t_perplexity_lst = [math.exp(x) for x in t_loss_list]\n",
    "  \n",
    "  # summarize history for accuracy\n",
    "  plt.plot(t_epochs_list, t_acc_list)\n",
    "  plt.plot(np.arange(0, len(val_perplexity_lst), 1), val_categorical_accuracy_list)\n",
    "  plt.title('model categorical accuracy for {}'.format(next(iter(net_type))))\n",
    "  plt.ylabel('categorical accuracy')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'valid'], loc='upper left')\n",
    "  plt.show()\n",
    "  \n",
    "  plt.plot(t_epochs_list, t_perplexity_lst)\n",
    "  plt.plot(np.arange(0, len(val_perplexity_lst), 1),val_perplexity_lst)\n",
    "  plt.title('model perplexity for {}'.format(next(iter(net_type))))\n",
    "  plt.ylabel('perplexity')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'valid'], loc='upper left')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgVaeSjoTvHr",
    "colab_type": "text"
   },
   "source": [
    "### Load model and check test final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "DQu4NO8USZy6",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def final_perplexity_per_model_type(net_type, data, vocabulary, num_steps, batch_size, checkpoint_path, is_test=True):\n",
    "  # build & compile the model for the training\n",
    "  new_model = None\n",
    "  if SAVE_RESULTS_TO_DRIVE:\n",
    "    new_model = load_model_from_drive(net_type)\n",
    "  else:\n",
    "    new_model = build_model(net_type, vocabulary)\n",
    "    new_model.load_weights(checkpoint_path)\n",
    "  \n",
    "  # setup data\n",
    "  data_generator = BatchGenerator(data, num_steps, batch_size, vocabulary)\n",
    "  \n",
    "  loss, acc = new_model.evaluate_generator(data_generator.generate(), steps=len(data)//(batch_size*num_steps))\n",
    "  print(\"Restored model, perplexity: {:5.2f}\".format(math.exp(loss)))\n",
    "  if is_test:\n",
    "    print(\"------------------------ Done test stage: {} ------------------------\".format(net_type))\n",
    "  else:\n",
    "    print(\"------------------------ Done validate stage: {} ------------------------\".format(net_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44Iijt0jpeup",
    "colab_type": "text"
   },
   "source": [
    "### Run Different Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Oo9CFLn9oHrg",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# get the data for the training\n",
    "train_data, valid_data, test_data, vocabulary, reversed_dictionary = load_data()\n",
    "\n",
    "# set parameters for the run\n",
    "num_steps = 20\n",
    "batch_size = 20\n",
    "\n",
    "# pick one to the following types\n",
    "type_1 = {\"L\":\"none\"}\n",
    "checkpoint_path_type_1 = None\n",
    "\n",
    "type_2 = {\"LD\":\"0.5\"}\n",
    "checkpoint_path_type_2 = None\n",
    "\n",
    "type_3 = {\"G\":\"none\"}\n",
    "checkpoint_path_type_3 = None\n",
    "\n",
    "type_4 = {\"GD\":\"0.5\"}\n",
    "checkpoint_path_type_4 = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctQsyIzfpqIg",
    "colab_type": "text"
   },
   "source": [
    "#### LSTM without Dropouts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtGp_lkvPhFb",
    "colab_type": "code",
    "outputId": "e6eeba3e-7455-466b-8ba5-1f2b8b670149",
    "executionInfo": {
     "status": "error",
     "timestamp": 1.545076471325E12,
     "user_tz": -120.0,
     "elapsed": 6213572.0,
     "user": {
      "displayName": "Aviv Notovich",
      "photoUrl": "",
      "userId": "02968534924654162048"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3618.0
    }
   },
   "outputs": [],
   "source": [
    "t_loss_list, t_acc_list, t_epochs_list = [], [], []\n",
    "checkpoint_path_type_1, history_1 = train_type(type_1, train_data, valid_data, num_steps, batch_size, vocabulary, epochs=25)\n",
    "print(\"Model saved into: {}\".format(checkpoint_path_type_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "cb1_pbB2dVCK",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "plot_graphs(type_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsL9X8PXcLQT",
    "colab_type": "code",
    "outputId": "bb85a5dc-2a05-40a2-ae3e-04a227b8edd6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.545045377618E12,
     "user_tz": -120.0,
     "elapsed": 17518.0,
     "user": {
      "displayName": "Aviv Notovich",
      "photoUrl": "",
      "userId": "02968534924654162048"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69.0
    }
   },
   "outputs": [],
   "source": [
    "# for validate\n",
    "final_perplexity_per_model_type(type_1, valid_data, vocabulary, num_steps, batch_size, checkpoint_path_type_1, is_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QFVQ0RABqv1J",
    "colab_type": "code",
    "outputId": "52d360df-5d29-4e58-9cde-46fb6e89de57",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.544998279393E12,
     "user_tz": -120.0,
     "elapsed": 15333.0,
     "user": {
      "displayName": "Aviv Notovich",
      "photoUrl": "",
      "userId": "02968534924654162048"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52.0
    }
   },
   "outputs": [],
   "source": [
    "# for test\n",
    "final_perplexity_per_model_type(type_1, test_data, vocabulary, num_steps, batch_size, checkpoint_path_type_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bD4woiHspym-",
    "colab_type": "text"
   },
   "source": [
    "#### LSTM with Dropouts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Cwrm2_oTAP0H",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "t_loss_list, t_acc_list, t_epochs_list = [], [], []\n",
    "checkpoint_path_type_2, history_2 = train_type(type_2, train_data, valid_data, num_steps, batch_size, vocabulary, epochs=25)\n",
    "print(\"Model saved into: {}\".format(checkpoint_path_type_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "RgJP0_3mqbS8",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# graphs 2\n",
    "plot_graphs(history_2, type_2, t_loss_list, t_acc_list, t_epochs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "M1-TlYgVcdsx",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# for validate\n",
    "final_perplexity_per_model_type(type_2, valid_data, vocabulary, num_steps, batch_size, checkpoint_path_type_2, is_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "jzmcCR4xcdiR",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# for test\n",
    "final_perplexity_per_model_type(type_2, test_data, vocabulary, num_steps, batch_size, checkpoint_path_type_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3hkv4Jpp0Pq",
    "colab_type": "text"
   },
   "source": [
    "#### GRU without Dropouts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "xQHTc-c8AW-t",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "t_loss_list, t_acc_list, t_epochs_list = [], [], []\n",
    "checkpoint_path_type_3, history_3 = train_type(type_3, train_data, valid_data, test_data, num_steps, batch_size, vocabulary)\n",
    "print(\"Model saved into: {}\".format(checkpoint_path_type_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "y399Iku-dZiA",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "plot_graphs(history_3, type_3, t_loss_list, t_acc_list, t_epochs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "M5PkYZKtc5O0",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# for validate\n",
    "final_perplexity_per_model_type(type_3, valid_data, vocabulary, num_steps, batch_size, checkpoint_path_type_3, is_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "r6gyxCrDc5Ri",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# for test\n",
    "final_perplexity_per_model_type(type_3, test_data, vocabulary, num_steps, batch_size, checkpoint_path_type_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UZarTSLp1yQ",
    "colab_type": "text"
   },
   "source": [
    "#### GRU with Dropouts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "fZAeN277AY9w",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "t_loss_list, t_acc_list, t_epochs_list = [], [], []\n",
    "checkpoint_path_type_4, history_4 = train_type(type_4, train_data, valid_data, test_data, num_steps, batch_size, vocabulary, epochs=40)\n",
    "print(\"Model saved into: {}\".format(checkpoint_path_type_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "uAe1HXIxdgR9",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "plot_graphs(history_4, type_4, t_loss_list, t_acc_list, t_epochs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "bmN_vcEYdB0F",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# for validate\n",
    "final_perplexity_per_model_type(type_4, valid_data, vocabulary, num_steps, batch_size, checkpoint_path_type_4, is_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "moelny25dBlz",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# for test\n",
    "final_perplexity_per_model_type(type_4, test_data, vocabulary, num_steps, batch_size, checkpoint_path_type_4)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ex2_036894996_201325750.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
